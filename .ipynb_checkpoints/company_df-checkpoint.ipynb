{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I would like to share how to process the work with text to build topics. To make it more attractive - apply NLP to the Stock Descriptions.\n",
    "\n",
    "We will load and build the models with cool 3d visualization.\n",
    "\n",
    "I encourage you to clone this notebook and play with every parameter to fine tune the result as your sticky mind would like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Befoure processing the data - run these command to load the stock information. (No SMS and registratoin neede)\n",
    "\n",
    "That script should create the diretories with of ditories with stock description.\n",
    "\n",
    "These files has basic company imformation like, company name, description, what exchanges they are places, web site etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADVM 200\n",
      "AB 200\n",
      "ADX 200\n",
      "ABAC 200\n",
      "ADXS 200\n",
      "AE 200\n",
      "ABB 200\n",
      "AEB 200\n",
      "ABBV 200\n",
      "AED 200\n",
      "ABC 200\n",
      "AEE 200\n",
      "ABCB 200\n",
      "AEF 200\n",
      "ABCD 200\n",
      "AEG 200\n",
      "ABDC 200\n",
      "AEGN 200\n",
      "ABEO 200\n",
      "AEH 200\n",
      "ABEOW 200\n",
      "AEHR 200\n",
      "ABEV 200\n",
      "AEIS 200\n",
      "AEL 200\n",
      "ABG 200\n",
      "ABIL 200\n",
      "AEM 200\n",
      "AEMD 200\n",
      "ABIO 200\n",
      "ABM 200\n",
      "AEO 200\n",
      "AEP 200\n",
      "ABMD 200\n",
      "AER 200\n",
      "ABR 200\n",
      "AERI 200\n",
      "ABR-A 200\n",
      "AES 200\n",
      "ABR-B 200\n",
      "AET 200\n",
      "ABR-C 200\n",
      "AETI 200\n",
      "ABT 200\n",
      "AEY 200\n",
      "ABTX 200\n",
      "AEYE 200\n",
      "AEZS 200\n",
      "AFB 200\n",
      "AFC 200\n",
      "ABUS 200\n",
      "AFG 200\n",
      "ABX 200\n",
      "AFGE 200\n",
      "AFGH 200\n",
      "AFH 200\n",
      "AFHBL 200\n",
      "AFI 200\n",
      "AFIF 200\n",
      "AFIN 200\n",
      "AFK 200\n",
      "AFL 200\n",
      "AFMD 200\n",
      "AFSI 200\n",
      "AFSI-A 200\n",
      "AFSI-B 200\n",
      "AFSI-C 200\n",
      "AFSI-D 200\n",
      "AFSI-E 200\n",
      "AFSI-F 200\n",
      "AFSS 200\n",
      "AFST 200\n",
      "AFT 200\n",
      "AFTY 200\n",
      "AG 200\n",
      "AGCO 200\n",
      "AGD 200\n",
      "AGEN 200\n",
      "AGF 200\n",
      "AGFS 200\n",
      "AGFSW 200\n",
      "AGG 200\n",
      "AGGE 200\n",
      "AGGP 200\n",
      "AGGY 200\n",
      "AGI 200\n",
      "AGIO 200\n",
      "AGLE 200\n",
      "AGM 200\n",
      "AGM-A 200\n",
      "AGM-B 200\n",
      "AGM-C 200\n",
      "AGM.A 200\n",
      "AGMH 200\n",
      "AGN 200\n",
      "AGNC 200\n",
      "AGNCB 200\n",
      "AGNCN 200\n",
      "AGND 200\n",
      "AGO 200\n",
      "AGO-B 200\n",
      "AGO-E 200\n",
      "AGO-F 200\n",
      "AGQ 200\n",
      "AGR 200\n",
      "AGRO 200\n",
      "AGRX 200\n",
      "AGS 200\n",
      "AGT 200\n",
      "AGTC 200\n",
      "AGX 200\n",
      "AGYS 200\n",
      "AGZ 200\n",
      "AGZD 200\n",
      "AHC 200\n",
      "AHH 200\n",
      "AHL 200\n",
      "AHL-C 200\n",
      "AHL-D 200\n",
      "AHPA 200\n",
      "AHPAU 200\n",
      "AHPAW 200\n",
      "AHPI 200\n",
      "AHT 200\n",
      "AHT-D 200\n",
      "AHT-F 200\n",
      "AHT-G 200\n",
      "AHT-H 200\n",
      "AHT-I 200\n",
      "AI 200\n",
      "AI-B 200\n",
      "AIA 200\n",
      "AIC 200\n",
      "AIEQ 200\n",
      "CPU times: user 502 ms, sys: 150 ms, total: 652 ms\n",
      "Wall time: 23.7 s\n"
     ]
    }
   ],
   "source": [
    "# around 10 minutes\n",
    "!python loader-company.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# work with file system\n",
    "import glob\n",
    "\n",
    "# paser the json text into dict\n",
    "import json\n",
    "\n",
    "companies = [json.load(open(filename,'r')) for filename in glob.iglob('company/**/*.json', recursive=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's put companies to Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CEO</th>\n",
       "      <th>companyName</th>\n",
       "      <th>description</th>\n",
       "      <th>exchange</th>\n",
       "      <th>industry</th>\n",
       "      <th>issueType</th>\n",
       "      <th>sector</th>\n",
       "      <th>symbol</th>\n",
       "      <th>tags</th>\n",
       "      <th>website</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yiding Sun</td>\n",
       "      <td>RISE Education Cayman Ltd</td>\n",
       "      <td>RISE Education Cayman Ltd through its subsidia...</td>\n",
       "      <td>NASDAQ Global Market</td>\n",
       "      <td>Education</td>\n",
       "      <td>cs</td>\n",
       "      <td>Consumer Defensive</td>\n",
       "      <td>REDU</td>\n",
       "      <td>[Consumer Defensive, Education &amp; Training Serv...</td>\n",
       "      <td>http://www.risecenter.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>SPDR DJ Wilshire Intl Real Estate</td>\n",
       "      <td>The investment seeks to provide investment res...</td>\n",
       "      <td>NYSE Arca</td>\n",
       "      <td></td>\n",
       "      <td>et</td>\n",
       "      <td></td>\n",
       "      <td>RWX</td>\n",
       "      <td>[]</td>\n",
       "      <td>http://www.spdrs.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kelly Hoffman</td>\n",
       "      <td>Ring Energy Inc.</td>\n",
       "      <td>Ring Energy Inc is a Midland-based exploration...</td>\n",
       "      <td>NYSE American</td>\n",
       "      <td>Oil &amp; Gas - E&amp;P</td>\n",
       "      <td>cs</td>\n",
       "      <td>Energy</td>\n",
       "      <td>REI</td>\n",
       "      <td>[Energy, Oil &amp; Gas E&amp;P, Oil &amp; Gas - E&amp;P]</td>\n",
       "      <td>http://www.ringenergy.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rajendra Dinkar Ketkar</td>\n",
       "      <td>Arcadia Biosciences Inc.</td>\n",
       "      <td>Arcadia Biosciences Inc is an agricultural bio...</td>\n",
       "      <td>NASDAQ Capital Market</td>\n",
       "      <td>Agriculture</td>\n",
       "      <td>cs</td>\n",
       "      <td>Basic Materials</td>\n",
       "      <td>RKDA</td>\n",
       "      <td>[Basic Materials, Agricultural Inputs, Agricul...</td>\n",
       "      <td>http://www.arcadiabio.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>Regalwood Global Energy Ltd. Units</td>\n",
       "      <td>Regalwood Global Energy Ltd is a newly organiz...</td>\n",
       "      <td>New York Stock Exchange</td>\n",
       "      <td>Conglomerates</td>\n",
       "      <td>su</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>RWGE=</td>\n",
       "      <td>[Industrials, Conglomerates]</td>\n",
       "      <td>https://www.carlyle.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      CEO                         companyName  \\\n",
       "0              Yiding Sun           RISE Education Cayman Ltd   \n",
       "1                           SPDR DJ Wilshire Intl Real Estate   \n",
       "2           Kelly Hoffman                    Ring Energy Inc.   \n",
       "3  Rajendra Dinkar Ketkar            Arcadia Biosciences Inc.   \n",
       "4                          Regalwood Global Energy Ltd. Units   \n",
       "\n",
       "                                         description                 exchange  \\\n",
       "0  RISE Education Cayman Ltd through its subsidia...     NASDAQ Global Market   \n",
       "1  The investment seeks to provide investment res...                NYSE Arca   \n",
       "2  Ring Energy Inc is a Midland-based exploration...            NYSE American   \n",
       "3  Arcadia Biosciences Inc is an agricultural bio...    NASDAQ Capital Market   \n",
       "4  Regalwood Global Energy Ltd is a newly organiz...  New York Stock Exchange   \n",
       "\n",
       "          industry issueType              sector symbol  \\\n",
       "0        Education        cs  Consumer Defensive   REDU   \n",
       "1                         et                        RWX   \n",
       "2  Oil & Gas - E&P        cs              Energy    REI   \n",
       "3      Agriculture        cs     Basic Materials   RKDA   \n",
       "4    Conglomerates        su         Industrials  RWGE=   \n",
       "\n",
       "                                                tags  \\\n",
       "0  [Consumer Defensive, Education & Training Serv...   \n",
       "1                                                 []   \n",
       "2           [Energy, Oil & Gas E&P, Oil & Gas - E&P]   \n",
       "3  [Basic Materials, Agricultural Inputs, Agricul...   \n",
       "4                       [Industrials, Conglomerates]   \n",
       "\n",
       "                     website  \n",
       "0  http://www.risecenter.com  \n",
       "1       http://www.spdrs.com  \n",
       "2  http://www.ringenergy.com  \n",
       "3  http://www.arcadiabio.com  \n",
       "4    https://www.carlyle.com  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(companies)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill N/A (empty items) in DataFrame with empty string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP\n",
    "\n",
    "I want to convert descriptions into something meaninfull so we can find companies with same semantics description.\n",
    "\n",
    "To acheave this goal I will work with NLP libraries to convert text to numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatizer - is process to convert works into thier simple form and get their pard of speac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemamatiza\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import gensim\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words like - be, is, the are need to be removed from sentense bacause they do not have any value of meaning and they would influence on result in bad way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at, because, been, over, couldn, theirs, ain, it, should, hasn, up, won't, being, by, few, just, below, be, yourselves, weren't, while, its, have, doesn't, so, those, mustn't, then, haven, from, nor, which, i, couldn't, them, only, d, ours, ll, s, were, very, doing, same, can, o, m, has, am, when, didn't, why, now, with, more, needn't, ve, but, other, herself, hadn, needn, himself, these, you're, here, weren, you, their, her, his, during, again, on, both, no, will, won, above, hadn't, that'll, wouldn, haven't, to, isn, having, of, how, against, hasn't, off, and, too, you'd, y, shouldn, once, an, don't, about, aren, they, what, shan, don, him, your, hers, the, there, than, in, he, such, each, yourself, does, if, wasn't, not, this, or, through, for, shouldn't, own, we, yours, ma, had, as, she, wasn, wouldn't, our, she's, my, ourselves, where, me, didn, down, all, between, that, re, itself, are, was, whom, isn't, is, into, doesn, t, should've, mightn't, shan't, out, a, you've, aren't, before, further, themselves, it's, any, mustn, after, until, you'll, did, do, who, under, most, some, myself, mightn\n"
     ]
    }
   ],
   "source": [
    "print(', '.join(stopWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']\n",
    "\n",
    "def is_allowed(token):\n",
    "    return (token.pos_ in allowed_postags) and (token.lemma_ not in stopWords)\n",
    "\n",
    "def lemmatization(texts):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append(\" \".join([token.lemma_ if token.lemma_ not in ['-PRON-'] else '' for token in doc if is_allowed(token)]))\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-9c86f5b3f74e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32myield\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimple_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeacc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# deacc=True removes punctuations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdata_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_to_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'description'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-9c86f5b3f74e>\u001b[0m in \u001b[0;36msent_to_words\u001b[0;34m(sentences)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msent_to_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0;32myield\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimple_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeacc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# deacc=True removes punctuations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdata_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_to_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'description'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36msimple_preprocess\u001b[0;34m(doc, deacc, min_len, max_len)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \"\"\"\n\u001b[1;32m    304\u001b[0m     tokens = [\n\u001b[0;32m--> 305\u001b[0;31m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeacc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeacc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmin_len\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mmax_len\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     ]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(df['description']))\n",
    "\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_lemmatized = lemmatization(data_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original vs Limmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('1.',df.iloc[0]['description'])\n",
    "print('1.', data_lemmatized[0])\n",
    "print('--')\n",
    "print('2.',df.iloc[1]['description'])\n",
    "print('2.', data_lemmatized[1])\n",
    "print('--')\n",
    "print('3.',df.iloc[2]['description'])\n",
    "print('3.', data_lemmatized[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Convert a collection of text documents to a matrix of token counts\n",
    "\n",
    "This implementation produces a sparse representation of the counts using scipy.sparse.csr_matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word',       \n",
    "                             min_df=10,                        # minimum reqd occurences of a word \n",
    "                             stop_words='english',             # remove stop words\n",
    "                             max_df=.09,                        # do not include vey\n",
    "                             lowercase=True,                   # convert all words to lowercase\n",
    "                             token_pattern='[a-zA-Z0-9]{3,}'  # num chars > 3\n",
    "                            )\n",
    "\n",
    "data_vectorized = vectorizer.fit_transform(data_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of the matrix\n",
    "data_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Materialize the sparse data\n",
    "data_dense = data_vectorized.todense()\n",
    "\n",
    "# Compute Sparsicity = Percentage of Non-Zero cells\n",
    "print(\"Sparsicity: \", ((data_dense > 0).sum()/data_dense.size)*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 1.18% of matrix has infomation... allot of zeros there. but that is the nature of our text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is THE Core of our algorithm. This black-box-machine can build us the Number Topics for given text.\n",
    "There are many ways to find the optimala number.\n",
    "\n",
    "I googled that there are 12 major industries https://www.thebalance.com/what-are-the-sectors-and-industries-of-the-sandp-500-3957507\n",
    "\n",
    "Somethimes company may by involved in many industries, they do sales and cloud computing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['symbol'] == 'BABA') | (df['symbol'] == 'REI') | (df['symbol'] == 'GOGL')][['symbol','industry']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would make sense to double the ammount of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Build LDA Model\n",
    "lda_model = LatentDirichletAllocation(n_components=24,           # Number of topics\n",
    "                                      max_iter=10,               # Max learning iterations\n",
    "                                      learning_method='online',   \n",
    "                                      random_state=100,          # Random state\n",
    "                                      batch_size=128,            # n docs in each learning iter\n",
    "                                      evaluate_every = -1,       # compute perplexity every n iters, default: Don't\n",
    "                                      n_jobs = -1,               # Use all available CPUs\n",
    "                                     )\n",
    "\n",
    "lda_output = lda_model.fit_transform(data_vectorized)\n",
    "\n",
    "print(lda_model)  # Model attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labling\n",
    "\n",
    "What we have done so far\n",
    "- Loaded the stocks\n",
    "- Cleaned the descriptions\n",
    "- Converted descriptions into Vectors\n",
    "- Build 24 topics for vectors via LDA model\n",
    "\n",
    "Now we need to:\n",
    "- Go throught each Vectors (stock description)\n",
    "- Feed them into LDA model\n",
    "- For each vector we will receive it's relation to all 12 topics ( 50% topic1, 10%topic2 ... 0% Topic24 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_output = lda_model.transform(data_vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... Done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lda_output.shape) # 8,752 Stocks\n",
    "print(lda_output[:2]) # first 2 stocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the result as table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# column names\n",
    "topicnames = [\"Topic\" + str(i) for i in range(lda_model.n_components)]\n",
    "\n",
    "# index names\n",
    "docnames = [i for i in df.symbol]\n",
    "\n",
    "# Make the pandas dataframe\n",
    "df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n",
    "\n",
    "# Get dominant topic for each document\n",
    "dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "df_document_topic['dominant_topic'] = dominant_topic\n",
    "\n",
    "# Styling\n",
    "def color_green(val):\n",
    "    color = 'green' if val > .05 else 'black'\n",
    "    return 'color: {col}'.format(col=color)\n",
    "\n",
    "def make_bold(val):\n",
    "    weight = 700 if val > .1 else 400\n",
    "    return 'font-weight: {weight}'.format(weight=weight)\n",
    "\n",
    "# Apply Style\n",
    "df_style_document_topics = df_document_topic.head(15).style.applymap(color_green).applymap(make_bold)\n",
    "df_style_document_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_document_topic['dominant_topic'].hist(bins=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topic distribution look ok. The spike on 17th topic not that huge.\n",
    "\n",
    "Yes, you can spend allto of time to make it better and better (not perfect)... not now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization\n",
    "\n",
    "We done with processing the text.\n",
    "\n",
    "Now let's visualize it with bokeh chart. (2d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove teh last column\n",
    "df_document_topic.drop(columns=\"dominant_topic\", inplace=True)\n",
    "df_document_topic.values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to put this 12 parameters/dimentions on plot - we need to convert them to 2d/3d.\n",
    "\n",
    "Fot that porpuse we will use the Dimentionality Reduction model. \n",
    "I will t-SNE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.manifold import TSNE\n",
    "topics_tsne = TSNE(perplexity=30, n_components=3)\n",
    "topics_tsne_vectors = topics_tsne.fit_transform(df_document_topic.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_tsne_vectors[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the data for Chart - X, Y, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topics_tsne_vectors = pd.DataFrame(topics_tsne_vectors,\n",
    "                            index=pd.Index(df_document_topic.index),\n",
    "                            columns=[u'x_coord', u'y_coord', u'z_coord'])\n",
    "df_topics_tsne_vectors['symb'] = df_topics_tsne_vectors.index\n",
    "df_topics_tsne_vectors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import HoverTool, ColumnDataSource, value\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bokeh plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add our DataFrame as a ColumnDataSource for Bokeh\n",
    "plot_data = ColumnDataSource(df_topics_tsne_vectors)\n",
    "\n",
    "# create the plot and configure the\n",
    "# title, dimensions, and tools\n",
    "tsne_plot = figure(title=u'Stocks NLP',\n",
    "                   plot_width = 800,\n",
    "                   plot_height = 800,\n",
    "                   tools= (u'pan, wheel_zoom, box_zoom,'\n",
    "                           u'box_select, reset'),\n",
    "                   active_scroll=u'wheel_zoom')\n",
    "\n",
    "# add a hover tool to display words on roll-over\n",
    "tsne_plot.add_tools( HoverTool(tooltips = u'@symb') )\n",
    "\n",
    "# draw the words as circles on the plot\n",
    "tsne_plot.circle(u'x_coord', u'y_coord', source=plot_data,\n",
    "                 color=u'blue', line_alpha=0.2, fill_alpha=0.1,\n",
    "                 size=10, hover_line_color=u'black')\n",
    "\n",
    "# configure visual elements of the plot\n",
    "tsne_plot.title.text_font_size = value(u'16pt')\n",
    "tsne_plot.xaxis.visible = False\n",
    "tsne_plot.yaxis.visible = False\n",
    "tsne_plot.grid.grid_line_color = None\n",
    "tsne_plot.outline_line_color = None\n",
    "\n",
    "# engage!\n",
    "show(tsne_plot);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, chart above looks cool. But it's hard to estimage if our result makes sense - Because our goal was to show how stocks are look-a-like to each other.\n",
    "\n",
    "It's hard to say - because we are using wrong tool (actualy I just don't know how to do that with bokeh... but!).\n",
    "\n",
    "Let look at the data in 3d with text search! For that we will use the https://projector.tensorflow.org \n",
    "\n",
    "Preperate the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topics_tsne_vectors.drop(columns='symb').to_csv('stock_vectors.tsv', sep='\\t', index=False, header=False )\n",
    "('@'+df['symbol']+ ' ' +df['description']).str.replace('\\n','.').to_csv('stock_metadata.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explorer the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/-BgjXXmt0V4?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
